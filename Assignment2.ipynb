{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computer vision artificial intelligence machine learning machine learning artificial intelligence computer vision computer vision machine learning computer vision artificial intelligence machine learning computer vision machine learning artificial intelligence machine learning computer vision artificial intelligence machine learning computer vision artificial intelligence machine learning computer vision artificial intelligence machine learning machine learning computer vision artificial intelligence artificial intelligence machine learning machine learning machine learning computer vision computer vision machine learning artificial intelligence computer vision artificial intelligence artificial intelligence machine learning artificial intelligence artificial intelligence computer vision artificial intelligence artificial intelligence artificial intelligence computer vision machine learning computer vision machine learning computer vision machine learning artificial intelligence artificial intelligence machine learning computer vision computer vision computer vision artificial intelligence machine learning computer vision machine learning artificial intelligence computer vision computer vision artificial intelligence computer vision machine learning computer vision computer vision computer vision computer vision computer vision machine learning artificial intelligence machine learning machine learning artificial intelligence computer vision artificial intelligence computer vision artificial intelligence computer vision machine learning', 'machine learning computer vision computer vision artificial intelligence computer vision machine learning artificial intelligence computer vision artificial intelligence computer vision computer vision artificial intelligence artificial intelligence artificial intelligence machine learning machine learning artificial intelligence artificial intelligence computer vision computer vision machine learning artificial intelligence computer vision artificial intelligence computer vision artificial intelligence computer vision machine learning machine learning', 'machine learning artificial intelligence computer vision machine learning computer vision artificial intelligence computer vision computer vision artificial intelligence artificial intelligence computer vision machine learning machine learning computer vision machine learning artificial intelligence artificial intelligence artificial intelligence computer vision machine learning computer vision artificial intelligence computer vision computer vision machine learning machine learning artificial intelligence machine learning machine learning machine learning artificial intelligence computer vision machine learning artificial intelligence artificial intelligence computer vision artificial intelligence computer vision machine learning computer vision computer vision computer vision machine learning artificial intelligence computer vision machine learning machine learning machine learning machine learning artificial intelligence artificial intelligence computer vision computer vision artificial intelligence machine learning machine learning computer vision artificial intelligence artificial intelligence machine learning machine learning artificial intelligence artificial intelligence machine learning artificial intelligence artificial intelligence machine learning machine learning artificial intelligence artificial intelligence artificial intelligence artificial intelligence machine learning artificial intelligence artificial intelligence machine learning computer vision artificial intelligence machine learning artificial intelligence machine learning machine learning artificial intelligence machine learning artificial intelligence machine learning artificial intelligence computer vision computer vision artificial intelligence artificial intelligence computer vision machine learning machine learning artificial intelligence artificial intelligence']\n",
      "[{'artificial': 0.3822702792057334, 'computer': 0.4557837944376052, 'intelligence': 0.3822702792057334, 'learning': 0.3822702792057334, 'machine': 0.3822702792057334, 'vision': 0.4557837944376052}, {'artificial': 0.45596466975932304, 'computer': 0.45596466975932304, 'intelligence': 0.45596466975932304, 'learning': 0.2901593353013874, 'machine': 0.2901593353013874, 'vision': 0.45596466975932304}, {'artificial': 0.4767913325670708, 'computer': 0.3011313679370973, 'intelligence': 0.4767913325670708, 'learning': 0.42660277124422125, 'machine': 0.42660277124422125, 'vision': 0.3011313679370973}]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "# Function to generate documents\n",
    "def generate_documents(phrases, num_docs):\n",
    "    documents = []\n",
    "    for _ in range(num_docs):\n",
    "        doc = \" \".join(np.random.choice(phrases, np.random.randint(5, 100)))\n",
    "        documents.append(doc)\n",
    "    return documents\n",
    "\n",
    "# Sample phrases\n",
    "phrases = [\"machine learning\",\"computer vision\",\"artificial intelligence\"]\n",
    "\n",
    "# Generate documents\n",
    "documents = generate_documents(phrases, 3)\n",
    "print(documents)\n",
    "# Step 1: Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "# Step 2: Fit and transform documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Step 3: Get feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Step 4: Convert TFIDF matrix to a list of dictionaries\n",
    "tfidf_documents_sklearn = []\n",
    "for i in range(len(documents)):\n",
    "    doc_vector = tfidf_matrix[i].toarray().flatten()\n",
    "    tfidf_doc = {feature_names[j]: doc_vector[j] for j in range(len(feature_names))}\n",
    "    tfidf_documents_sklearn.append(tfidf_doc)\n",
    "\n",
    "print(tfidf_documents_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'computer': 0.45578379443760525, 'vision': 0.45578379443760525, 'artificial': 0.3822702792057335, 'intelligence': 0.3822702792057335, 'machine': 0.3822702792057335, 'learning': 0.3822702792057335}, {'machine': 0.29015933530138743, 'learning': 0.29015933530138743, 'computer': 0.45596466975932304, 'vision': 0.45596466975932304, 'artificial': 0.45596466975932304, 'intelligence': 0.45596466975932304}, {'machine': 0.42660277124422125, 'learning': 0.42660277124422125, 'artificial': 0.4767913325670708, 'intelligence': 0.4767913325670708, 'computer': 0.3011313679370973, 'vision': 0.3011313679370973}]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def clean_data(document):\n",
    "    cleaned_doc = ''.join([char.lower() for char in document if char.isalnum() or char.isspace()])\n",
    "    return cleaned_doc\n",
    "\n",
    "cleaned_documents = [clean_data(doc) for doc in documents]\n",
    "\n",
    "# Step 2: Tokenization\n",
    "tokenized_documents = [word_tokenize(doc) for doc in cleaned_documents]\n",
    "\n",
    "# Step 3: Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_documents = [[lemmatizer.lemmatize(word) for word in doc] for doc in tokenized_documents]\n",
    "\n",
    "# Step 4: Remove stop words\n",
    "ENGLISH_STOP_WORDS = set(stopwords.words('english'))\n",
    "filtered_documents = [[word for word in doc if word not in ENGLISH_STOP_WORDS] for doc in lemmatized_documents]\n",
    "\n",
    "# Step 5: Get unique words\n",
    "unique_words = list(set([word for doc in filtered_documents for word in doc]))\n",
    "\n",
    "# Step 6: Compute TF\n",
    "def compute_tf(document):\n",
    "    word_counts = Counter(document)\n",
    "    total_words = len(document)\n",
    "    tf = {word: word_counts[word] / total_words for word in word_counts}\n",
    "    return tf\n",
    "\n",
    "tf_documents = [compute_tf(doc) for doc in filtered_documents]\n",
    "\n",
    "# Step 7: Compute IDF\n",
    "def compute_idf(documents, unique_words):\n",
    "    idf = {}\n",
    "    total_documents = len(documents)\n",
    "    for word in unique_words:\n",
    "        num_documents_containing_word = sum([1 for doc in documents if word in doc])\n",
    "        idf[word] = math.log((1 + total_documents) / (1 + num_documents_containing_word)) + 1\n",
    "    return idf\n",
    "\n",
    "idf = compute_idf(filtered_documents, unique_words)\n",
    "\n",
    "# Step 8: Compute TFIDF\n",
    "tfidf_documents = []\n",
    "for tf_doc in tf_documents:\n",
    "    tfidf_doc = {word: tf_doc[word] * idf[word] for word in tf_doc}\n",
    "    tfidf_documents.append(tfidf_doc)\n",
    "\n",
    "# Step 9: Normalize TFIDF\n",
    "def normalize_tfidf(tfidf_doc):\n",
    "    norm = np.linalg.norm(list(tfidf_doc.values()))\n",
    "    normalized_tfidf = {word: tfidf_doc[word] / norm for word in tfidf_doc}\n",
    "    return normalized_tfidf\n",
    "\n",
    "normalized_tfidf_documents = [normalize_tfidf(doc) for doc in tfidf_documents]\n",
    "\n",
    "print(normalized_tfidf_documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
